{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d333515a",
   "metadata": {},
   "source": [
    "<h1 align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "کتابدار برج بابِل</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed20bff",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "مقدمه و صورت مسئله\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    هدف این مسئله تشخیص برچسب‌های دسته‌بندی کتاب‌ها بر اساس خلاصه‌ی آن‌هاست. به این منظور، مجموعه‌ای از کتاب‌ها، خلاصه‌های آن‌ها و برچسب‌های آن‌ها در اختیار شما قرار گرفته است. هدف، ایجاد مدلی است که با دیدن یک خلاصه‌ی کتاب جدید حدس بزند که این کتاب شامل کدام برچسب‌ها می‌شود و کدام را شامل نمی‌شود.  </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af1784",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "وارد کردن کتابخانه‌های مورد نیاز\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    ابتدا کتابخانه‌های مورد نیازتان را وارد کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99450f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72faaed8",
   "metadata": {},
   "source": [
    "\n",
    "<h2 align=\"right\" style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=\"rtl\" style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=\"3\">\n",
    "مجموعه‌ داده شامل اطلاعات مربوط به ۳۹۶۶ کتاب است که شامل داده‌های آموزشی و نمونه‌های آزمون می‌باشد. اطلاعات موجود برای هر کتاب شامل نام کتاب، خلاصه‌ی آن و برچسب‌های مربوطه است.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=\"rtl\" style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=\"3\">\n",
    "داده‌های آموزشی در یک فایل با نام <code>train.csv</code> قرار دارد که شامل ۳۱۸۰ سطر می‌باشد. این فایل شامل ۹ دسته‌بندی است. قرار گرفتن در یک دسته‌بندی با عدد ۱ و عدم قرار گرفتن با عدد ۰ نشان داده می‌شود. توضیحات مربوط به ستون‌های مهم در این مسئله در جدول زیر آمده است.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<div dir=rtl style=\"direction: rtl;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "|ستون|توضیحات|\n",
    "|:------:|:---:|\n",
    "|title|عنوان کتاب یا مطلب|\n",
    "|description|توضیحات مربوط به کتاب|\n",
    "|Thriller|قرارگیری در دسته‌بندی تریلر|\n",
    "|Classics|قرارگیری در دسته‌بندی کلاسیک|\n",
    "|Romance|قرارگیری در دسته‌بندی عاشقانه|\n",
    "|Mystery|قرارگیری در دسته‌بندی معمایی|\n",
    "|Science|قرارگیری در دسته‌بندی علمی|\n",
    "|Literature|قرارگیری در دسته‌بندی ادبیات|\n",
    "|Fantasy|قرارگیری در دسته‌بندی فانتزی|\n",
    "|Historical|قرارگیری در دسته‌بندی تاریخی|\n",
    "|Fiction|قرارگیری در دسته‌بندی داستانی|\n",
    "    \n",
    "</font>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bbe9b",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "خواندن مجموعه داده\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    در ابتدا نیاز است فایل‌های مجموعه‌داده را بخوانید. نمونه‌های آموزشی در فایل <code>train.csv</code> و نمونه‌های آزمون که باید دسته‌بندی‌های آن‌ها را پیش‌بینی کنید در فایل <code>test.csv</code> ذخیره شده‌اند. اگر لازم دانستید می‌توانید به دلخواه خود بخشی از دادگان آموزشی را به عنوان دادگان اعتبارسنجی نیز جدا کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcb9e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Classics</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Science</th>\n",
       "      <th>Literature</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Historical</th>\n",
       "      <th>Fiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's Not About the Bike: My Journey Back to Life</td>\n",
       "      <td>It is such an all-American story. A lanky kid ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trilogía Involuntaria #3The House of the Spirits</td>\n",
       "      <td>In one of the most important and beloved Latin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Girl Who Drank the Moon</td>\n",
       "      <td>Every year, the people of the Protectorate lea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Give Me Tomorrow: The Korean War's Greatest Un...</td>\n",
       "      <td>An epic story of valor and sacrifice by a lege...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The World of Winnie-the-Pooh</td>\n",
       "      <td>In 1926, \"Winnie-the-Pooh,\" a collection of st...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>Wisdom Revolution #2The Oldest Dance</td>\n",
       "      <td>As one of the war heroes searches the oldest l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>Mercy #3Beg For Mercy</td>\n",
       "      <td>The fight is on in this installment, Mercy is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>Halo #1Halo</td>\n",
       "      <td>An angel is sent to Earth on a mission.But fal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>Genius #1Evil Genius</td>\n",
       "      <td>Cadel Piggott has a genius IQ and a fascinatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>Warriors: Omen of the Stars #6The Last Hope</td>\n",
       "      <td>The end of the stars draws near. Three must be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2953 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0      It's Not About the Bike: My Journey Back to Life   \n",
       "1      Trilogía Involuntaria #3The House of the Spirits   \n",
       "2                           The Girl Who Drank the Moon   \n",
       "3     Give Me Tomorrow: The Korean War's Greatest Un...   \n",
       "4                          The World of Winnie-the-Pooh   \n",
       "...                                                 ...   \n",
       "2948               Wisdom Revolution #2The Oldest Dance   \n",
       "2949                              Mercy #3Beg For Mercy   \n",
       "2950                                        Halo #1Halo   \n",
       "2951                               Genius #1Evil Genius   \n",
       "2952        Warriors: Omen of the Stars #6The Last Hope   \n",
       "\n",
       "                                            description  Thriller  Classics  \\\n",
       "0     It is such an all-American story. A lanky kid ...         0         0   \n",
       "1     In one of the most important and beloved Latin...         0         0   \n",
       "2     Every year, the people of the Protectorate lea...         0         0   \n",
       "3     An epic story of valor and sacrifice by a lege...         0         0   \n",
       "4     In 1926, \"Winnie-the-Pooh,\" a collection of st...         0         1   \n",
       "...                                                 ...       ...       ...   \n",
       "2948  As one of the war heroes searches the oldest l...         0         0   \n",
       "2949  The fight is on in this installment, Mercy is ...         0         0   \n",
       "2950  An angel is sent to Earth on a mission.But fal...         0         0   \n",
       "2951  Cadel Piggott has a genius IQ and a fascinatio...         0         0   \n",
       "2952  The end of the stars draws near. Three must be...         0         0   \n",
       "\n",
       "      Romance  Mystery  Science  Literature  Fantasy  Historical  Fiction  \n",
       "0           0        0        0           0        0           0        0  \n",
       "1           0        0        0           1        1           1        1  \n",
       "2           0        0        0           0        0           0        1  \n",
       "3           0        0        0           0        0           0        1  \n",
       "4           0        0        0           0        1           0        1  \n",
       "...       ...      ...      ...         ...      ...         ...      ...  \n",
       "2948        0        0        1           0        1           0        1  \n",
       "2949        1        0        0           0        0           0        1  \n",
       "2950        1        0        0           0        1           0        0  \n",
       "2951        0        1        1           0        1           0        1  \n",
       "2952        0        0        0           0        1           0        1  \n",
       "\n",
       "[2953 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/train.csv') \n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb6ba5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asian Saga: Chronological Order #2Tai-Pan</td>\n",
       "      <td>Set in the turbulent days of the founding of H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prince of Pain #1Prince of Pain I</td>\n",
       "      <td>When Liam pays a madam for a specific fantasy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Practical Magic #1Practical Magic</td>\n",
       "      <td>Alternate cover for ISBN 9780425190371 (curren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asian Saga: Chronological Order #5Noble House</td>\n",
       "      <td>This is an alternate cover edition for ISBN13:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El Diablo #1El Diablo</td>\n",
       "      <td>FROM WALL STREET JOURNAL &amp; USA BESTSELLING AUT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Maybe #1.5Maybe Not</td>\n",
       "      <td>Colleen Hoover, the New York Times bestselling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>The Saga of Darren Shan #11Lord of the Shadows</td>\n",
       "      <td>Book 11 of The Saga Of Darren Shan. Darren's g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>The Highly Sensitive Person: How to Thrive Whe...</td>\n",
       "      <td>Do you have a keen imagination and vivid dream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>Bad Girls Don't Die #3As Dead As It Gets</td>\n",
       "      <td>It's been three months since Alexis helplessly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>The Perfect Mother</td>\n",
       "      <td>An addictive psychological thriller about a gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0            Asian Saga: Chronological Order #2Tai-Pan   \n",
       "1                    Prince of Pain #1Prince of Pain I   \n",
       "2                    Practical Magic #1Practical Magic   \n",
       "3        Asian Saga: Chronological Order #5Noble House   \n",
       "4                                El Diablo #1El Diablo   \n",
       "..                                                 ...   \n",
       "734                                Maybe #1.5Maybe Not   \n",
       "735     The Saga of Darren Shan #11Lord of the Shadows   \n",
       "736  The Highly Sensitive Person: How to Thrive Whe...   \n",
       "737           Bad Girls Don't Die #3As Dead As It Gets   \n",
       "738                                 The Perfect Mother   \n",
       "\n",
       "                                           description  \n",
       "0    Set in the turbulent days of the founding of H...  \n",
       "1    When Liam pays a madam for a specific fantasy ...  \n",
       "2    Alternate cover for ISBN 9780425190371 (curren...  \n",
       "3    This is an alternate cover edition for ISBN13:...  \n",
       "4    FROM WALL STREET JOURNAL & USA BESTSELLING AUT...  \n",
       "..                                                 ...  \n",
       "734  Colleen Hoover, the New York Times bestselling...  \n",
       "735  Book 11 of The Saga Of Darren Shan. Darren's g...  \n",
       "736  Do you have a keen imagination and vivid dream...  \n",
       "737  It's been three months since Alexis helplessly...  \n",
       "738  An addictive psychological thriller about a gr...  \n",
       "\n",
       "[739 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23daec54",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "پیش‌پردازش و مهندسی ویژگی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    در این سوال شما می‌توانید از هر تکنیک پیش‌پردازش/مهندسی ویژگی که در گذشته آموختید، استفاده کنید.\n",
    "    <br>\n",
    "    تکنیک‌هایی که استفاده می‌کنید به شکل مستقیم مورد ارزیابی توسط سامانه داوری قرار <b>نمی‌گیرند.</b> بلکه همه آن‌ها در دقت مدل شما تاثیر خواهند گذاشت؛ بنابراین هر چه پیش‌پردازش/مهندسی ویژگی بهتری انجام دهید تا دقت مدل بهبود پیدا کند، امتیاز بیشتری از این سوال کسب خواهید کرد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50dae50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean nan rows\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "# tokenizing the text data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['description'])\n",
    "x_seq = tokenizer.texts_to_sequences(train_data['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f2c94",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "مدل‌سازی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    حال که داده را پاکسازی کرده و احتمالا ویژگی‌هایی را به آن افزوده یا از آن حذف کرده‌اید، وقت آن است که مدلی آموزش دهید که بتواند ستون‌های هدف این مسئله را پیش‌بینی کند.\n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d20a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "74/74 [==============================] - 39s 401ms/step - loss: 4.8779 - Thriller_loss: 0.4470 - Classics_loss: 0.5547 - Romance_loss: 0.6441 - Mystery_loss: 0.5226 - Science_loss: 0.4910 - Literature_loss: 0.4830 - Fantasy_loss: 0.6854 - Historical_loss: 0.5178 - Fiction_loss: 0.5323 - Thriller_accuracy: 0.8508 - Classics_accuracy: 0.7746 - Romance_accuracy: 0.6746 - Mystery_accuracy: 0.8064 - Science_accuracy: 0.8242 - Literature_accuracy: 0.8301 - Fantasy_accuracy: 0.5856 - Historical_accuracy: 0.8106 - Fiction_accuracy: 0.7970 - val_loss: 4.5466 - val_Thriller_loss: 0.4028 - val_Classics_loss: 0.5136 - val_Romance_loss: 0.6391 - val_Mystery_loss: 0.4537 - val_Science_loss: 0.4511 - val_Literature_loss: 0.4117 - val_Fantasy_loss: 0.6794 - val_Historical_loss: 0.4711 - val_Fiction_loss: 0.5241 - val_Thriller_accuracy: 0.8610 - val_Classics_accuracy: 0.7712 - val_Romance_accuracy: 0.6678 - val_Mystery_accuracy: 0.8322 - val_Science_accuracy: 0.8339 - val_Literature_accuracy: 0.8424 - val_Fantasy_accuracy: 0.5881 - val_Historical_accuracy: 0.8153 - val_Fiction_accuracy: 0.7864\n",
      "Epoch 2/30\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 4.3704 - Thriller_loss: 0.3848 - Classics_loss: 0.4472 - Romance_loss: 0.5775 - Mystery_loss: 0.4675 - Science_loss: 0.4522 - Literature_loss: 0.3784 - Fantasy_loss: 0.6778 - Historical_loss: 0.4585 - Fiction_loss: 0.5266 - Thriller_accuracy: 0.8627 - Classics_accuracy: 0.7856 - Romance_accuracy: 0.7174 - Mystery_accuracy: 0.8131 - Science_accuracy: 0.8369 - Literature_accuracy: 0.8398 - Fantasy_accuracy: 0.5898 - Historical_accuracy: 0.8186 - Fiction_accuracy: 0.7975 - val_loss: 4.1293 - val_Thriller_loss: 0.3743 - val_Classics_loss: 0.4136 - val_Romance_loss: 0.5018 - val_Mystery_loss: 0.4246 - val_Science_loss: 0.4546 - val_Literature_loss: 0.3301 - val_Fantasy_loss: 0.6438 - val_Historical_loss: 0.4544 - val_Fiction_loss: 0.5320 - val_Thriller_accuracy: 0.8610 - val_Classics_accuracy: 0.8068 - val_Romance_accuracy: 0.7695 - val_Mystery_accuracy: 0.8322 - val_Science_accuracy: 0.8339 - val_Literature_accuracy: 0.8525 - val_Fantasy_accuracy: 0.6356 - val_Historical_accuracy: 0.8153 - val_Fiction_accuracy: 0.7864\n",
      "Epoch 3/30\n",
      "74/74 [==============================] - 25s 334ms/step - loss: 3.7281 - Thriller_loss: 0.2942 - Classics_loss: 0.3117 - Romance_loss: 0.4404 - Mystery_loss: 0.3762 - Science_loss: 0.4465 - Literature_loss: 0.2871 - Fantasy_loss: 0.6090 - Historical_loss: 0.4339 - Fiction_loss: 0.5292 - Thriller_accuracy: 0.8678 - Classics_accuracy: 0.8640 - Romance_accuracy: 0.8110 - Mystery_accuracy: 0.8275 - Science_accuracy: 0.8369 - Literature_accuracy: 0.8682 - Fantasy_accuracy: 0.6691 - Historical_accuracy: 0.8169 - Fiction_accuracy: 0.7983 - val_loss: 3.9173 - val_Thriller_loss: 0.3296 - val_Classics_loss: 0.4319 - val_Romance_loss: 0.4580 - val_Mystery_loss: 0.3749 - val_Science_loss: 0.4539 - val_Literature_loss: 0.3136 - val_Fantasy_loss: 0.6027 - val_Historical_loss: 0.4348 - val_Fiction_loss: 0.5180 - val_Thriller_accuracy: 0.8746 - val_Classics_accuracy: 0.8034 - val_Romance_accuracy: 0.7881 - val_Mystery_accuracy: 0.8610 - val_Science_accuracy: 0.8339 - val_Literature_accuracy: 0.8559 - val_Fantasy_accuracy: 0.6898 - val_Historical_accuracy: 0.8153 - val_Fiction_accuracy: 0.7864\n",
      "Epoch 4/30\n",
      "74/74 [==============================] - 24s 323ms/step - loss: 2.9556 - Thriller_loss: 0.1873 - Classics_loss: 0.2227 - Romance_loss: 0.2995 - Mystery_loss: 0.2154 - Science_loss: 0.4071 - Literature_loss: 0.2092 - Fantasy_loss: 0.5375 - Historical_loss: 0.3935 - Fiction_loss: 0.4835 - Thriller_accuracy: 0.9258 - Classics_accuracy: 0.9102 - Romance_accuracy: 0.8864 - Mystery_accuracy: 0.9161 - Science_accuracy: 0.8369 - Literature_accuracy: 0.9110 - Fantasy_accuracy: 0.7237 - Historical_accuracy: 0.8250 - Fiction_accuracy: 0.7924 - val_loss: 3.8121 - val_Thriller_loss: 0.3136 - val_Classics_loss: 0.4070 - val_Romance_loss: 0.4577 - val_Mystery_loss: 0.3473 - val_Science_loss: 0.4475 - val_Literature_loss: 0.3020 - val_Fantasy_loss: 0.5926 - val_Historical_loss: 0.4340 - val_Fiction_loss: 0.5105 - val_Thriller_accuracy: 0.8864 - val_Classics_accuracy: 0.8288 - val_Romance_accuracy: 0.7814 - val_Mystery_accuracy: 0.8763 - val_Science_accuracy: 0.8339 - val_Literature_accuracy: 0.8678 - val_Fantasy_accuracy: 0.6983 - val_Historical_accuracy: 0.8119 - val_Fiction_accuracy: 0.7864\n",
      "Epoch 5/30\n",
      "74/74 [==============================] - 23s 316ms/step - loss: 2.3814 - Thriller_loss: 0.1275 - Classics_loss: 0.1959 - Romance_loss: 0.1797 - Mystery_loss: 0.1555 - Science_loss: 0.3613 - Literature_loss: 0.1797 - Fantasy_loss: 0.4167 - Historical_loss: 0.3694 - Fiction_loss: 0.3957 - Thriller_accuracy: 0.9534 - Classics_accuracy: 0.9165 - Romance_accuracy: 0.9403 - Mystery_accuracy: 0.9475 - Science_accuracy: 0.8424 - Literature_accuracy: 0.9267 - Fantasy_accuracy: 0.8081 - Historical_accuracy: 0.8301 - Fiction_accuracy: 0.8178 - val_loss: 3.8033 - val_Thriller_loss: 0.3586 - val_Classics_loss: 0.4404 - val_Romance_loss: 0.4476 - val_Mystery_loss: 0.3872 - val_Science_loss: 0.4166 - val_Literature_loss: 0.2936 - val_Fantasy_loss: 0.5114 - val_Historical_loss: 0.4246 - val_Fiction_loss: 0.5234 - val_Thriller_accuracy: 0.8864 - val_Classics_accuracy: 0.8237 - val_Romance_accuracy: 0.8000 - val_Mystery_accuracy: 0.8678 - val_Science_accuracy: 0.8339 - val_Literature_accuracy: 0.8712 - val_Fantasy_accuracy: 0.7712 - val_Historical_accuracy: 0.8102 - val_Fiction_accuracy: 0.7814\n",
      "Epoch 6/30\n",
      "74/74 [==============================] - 23s 309ms/step - loss: 1.8860 - Thriller_loss: 0.1037 - Classics_loss: 0.1929 - Romance_loss: 0.1179 - Mystery_loss: 0.1390 - Science_loss: 0.2936 - Literature_loss: 0.1716 - Fantasy_loss: 0.2431 - Historical_loss: 0.3424 - Fiction_loss: 0.2817 - Thriller_accuracy: 0.9559 - Classics_accuracy: 0.9195 - Romance_accuracy: 0.9585 - Mystery_accuracy: 0.9530 - Science_accuracy: 0.8678 - Literature_accuracy: 0.9275 - Fantasy_accuracy: 0.8992 - Historical_accuracy: 0.8441 - Fiction_accuracy: 0.8814 - val_loss: 3.8351 - val_Thriller_loss: 0.3584 - val_Classics_loss: 0.4240 - val_Romance_loss: 0.5351 - val_Mystery_loss: 0.3591 - val_Science_loss: 0.4090 - val_Literature_loss: 0.2961 - val_Fantasy_loss: 0.5028 - val_Historical_loss: 0.4193 - val_Fiction_loss: 0.5313 - val_Thriller_accuracy: 0.8864 - val_Classics_accuracy: 0.8237 - val_Romance_accuracy: 0.7983 - val_Mystery_accuracy: 0.8695 - val_Science_accuracy: 0.8475 - val_Literature_accuracy: 0.8814 - val_Fantasy_accuracy: 0.7593 - val_Historical_accuracy: 0.8051 - val_Fiction_accuracy: 0.7831\n",
      "Epoch 7/30\n",
      "74/74 [==============================] - 24s 326ms/step - loss: 1.5208 - Thriller_loss: 0.1076 - Classics_loss: 0.1764 - Romance_loss: 0.0807 - Mystery_loss: 0.0980 - Science_loss: 0.2311 - Literature_loss: 0.1614 - Fantasy_loss: 0.1591 - Historical_loss: 0.3090 - Fiction_loss: 0.1975 - Thriller_accuracy: 0.9610 - Classics_accuracy: 0.9203 - Romance_accuracy: 0.9742 - Mystery_accuracy: 0.9661 - Science_accuracy: 0.9021 - Literature_accuracy: 0.9314 - Fantasy_accuracy: 0.9428 - Historical_accuracy: 0.8585 - Fiction_accuracy: 0.9254 - val_loss: 4.0277 - val_Thriller_loss: 0.3511 - val_Classics_loss: 0.4508 - val_Romance_loss: 0.5348 - val_Mystery_loss: 0.4013 - val_Science_loss: 0.4043 - val_Literature_loss: 0.3015 - val_Fantasy_loss: 0.5512 - val_Historical_loss: 0.4205 - val_Fiction_loss: 0.6122 - val_Thriller_accuracy: 0.8881 - val_Classics_accuracy: 0.8186 - val_Romance_accuracy: 0.8017 - val_Mystery_accuracy: 0.8441 - val_Science_accuracy: 0.8390 - val_Literature_accuracy: 0.8797 - val_Fantasy_accuracy: 0.7542 - val_Historical_accuracy: 0.8102 - val_Fiction_accuracy: 0.7797\n",
      "Epoch 8/30\n",
      "74/74 [==============================] - 24s 324ms/step - loss: 1.2977 - Thriller_loss: 0.0979 - Classics_loss: 0.1535 - Romance_loss: 0.0687 - Mystery_loss: 0.1117 - Science_loss: 0.2015 - Literature_loss: 0.1361 - Fantasy_loss: 0.1346 - Historical_loss: 0.2585 - Fiction_loss: 0.1352 - Thriller_accuracy: 0.9597 - Classics_accuracy: 0.9301 - Romance_accuracy: 0.9758 - Mystery_accuracy: 0.9568 - Science_accuracy: 0.9123 - Literature_accuracy: 0.9360 - Fantasy_accuracy: 0.9453 - Historical_accuracy: 0.8852 - Fiction_accuracy: 0.9551 - val_loss: 4.2026 - val_Thriller_loss: 0.3685 - val_Classics_loss: 0.4823 - val_Romance_loss: 0.6043 - val_Mystery_loss: 0.3915 - val_Science_loss: 0.4426 - val_Literature_loss: 0.3095 - val_Fantasy_loss: 0.5390 - val_Historical_loss: 0.4309 - val_Fiction_loss: 0.6340 - val_Thriller_accuracy: 0.8881 - val_Classics_accuracy: 0.8237 - val_Romance_accuracy: 0.8000 - val_Mystery_accuracy: 0.8729 - val_Science_accuracy: 0.8322 - val_Literature_accuracy: 0.8831 - val_Fantasy_accuracy: 0.7898 - val_Historical_accuracy: 0.8034 - val_Fiction_accuracy: 0.7949\n",
      "Epoch 9/30\n",
      "74/74 [==============================] - 23s 316ms/step - loss: 1.1222 - Thriller_loss: 0.0951 - Classics_loss: 0.1390 - Romance_loss: 0.0603 - Mystery_loss: 0.0937 - Science_loss: 0.1641 - Literature_loss: 0.1321 - Fantasy_loss: 0.1265 - Historical_loss: 0.2145 - Fiction_loss: 0.0969 - Thriller_accuracy: 0.9619 - Classics_accuracy: 0.9398 - Romance_accuracy: 0.9814 - Mystery_accuracy: 0.9657 - Science_accuracy: 0.9292 - Literature_accuracy: 0.9432 - Fantasy_accuracy: 0.9483 - Historical_accuracy: 0.9110 - Fiction_accuracy: 0.9695 - val_loss: 4.4675 - val_Thriller_loss: 0.3827 - val_Classics_loss: 0.5602 - val_Romance_loss: 0.5973 - val_Mystery_loss: 0.4244 - val_Science_loss: 0.4542 - val_Literature_loss: 0.3440 - val_Fantasy_loss: 0.5760 - val_Historical_loss: 0.4458 - val_Fiction_loss: 0.6829 - val_Thriller_accuracy: 0.8847 - val_Classics_accuracy: 0.8203 - val_Romance_accuracy: 0.8068 - val_Mystery_accuracy: 0.8627 - val_Science_accuracy: 0.8508 - val_Literature_accuracy: 0.8746 - val_Fantasy_accuracy: 0.7780 - val_Historical_accuracy: 0.8068 - val_Fiction_accuracy: 0.7864\n",
      "Epoch 10/30\n",
      "74/74 [==============================] - 24s 328ms/step - loss: 0.9609 - Thriller_loss: 0.0900 - Classics_loss: 0.1381 - Romance_loss: 0.0388 - Mystery_loss: 0.0759 - Science_loss: 0.1494 - Literature_loss: 0.1107 - Fantasy_loss: 0.1052 - Historical_loss: 0.1788 - Fiction_loss: 0.0739 - Thriller_accuracy: 0.9636 - Classics_accuracy: 0.9386 - Romance_accuracy: 0.9890 - Mystery_accuracy: 0.9716 - Science_accuracy: 0.9394 - Literature_accuracy: 0.9492 - Fantasy_accuracy: 0.9547 - Historical_accuracy: 0.9246 - Fiction_accuracy: 0.9788 - val_loss: 4.8419 - val_Thriller_loss: 0.4159 - val_Classics_loss: 0.5672 - val_Romance_loss: 0.6905 - val_Mystery_loss: 0.3961 - val_Science_loss: 0.4522 - val_Literature_loss: 0.3404 - val_Fantasy_loss: 0.7450 - val_Historical_loss: 0.4374 - val_Fiction_loss: 0.7971 - val_Thriller_accuracy: 0.8864 - val_Classics_accuracy: 0.8102 - val_Romance_accuracy: 0.8051 - val_Mystery_accuracy: 0.8780 - val_Science_accuracy: 0.8153 - val_Literature_accuracy: 0.8898 - val_Fantasy_accuracy: 0.7356 - val_Historical_accuracy: 0.8136 - val_Fiction_accuracy: 0.7983\n",
      "Epoch 11/30\n",
      "74/74 [==============================] - 19s 264ms/step - loss: 0.8968 - Thriller_loss: 0.0868 - Classics_loss: 0.1290 - Romance_loss: 0.0426 - Mystery_loss: 0.0770 - Science_loss: 0.1305 - Literature_loss: 0.1137 - Fantasy_loss: 0.0880 - Historical_loss: 0.1573 - Fiction_loss: 0.0718 - Thriller_accuracy: 0.9665 - Classics_accuracy: 0.9445 - Romance_accuracy: 0.9860 - Mystery_accuracy: 0.9699 - Science_accuracy: 0.9483 - Literature_accuracy: 0.9517 - Fantasy_accuracy: 0.9661 - Historical_accuracy: 0.9347 - Fiction_accuracy: 0.9763 - val_loss: 4.5673 - val_Thriller_loss: 0.4108 - val_Classics_loss: 0.5650 - val_Romance_loss: 0.7054 - val_Mystery_loss: 0.4118 - val_Science_loss: 0.4251 - val_Literature_loss: 0.3381 - val_Fantasy_loss: 0.5817 - val_Historical_loss: 0.4223 - val_Fiction_loss: 0.7070 - val_Thriller_accuracy: 0.8864 - val_Classics_accuracy: 0.8271 - val_Romance_accuracy: 0.8119 - val_Mystery_accuracy: 0.8729 - val_Science_accuracy: 0.8356 - val_Literature_accuracy: 0.8847 - val_Fantasy_accuracy: 0.7695 - val_Historical_accuracy: 0.8271 - val_Fiction_accuracy: 0.7915\n",
      "Epoch 12/30\n",
      "74/74 [==============================] - 19s 260ms/step - loss: 0.7585 - Thriller_loss: 0.0733 - Classics_loss: 0.1039 - Romance_loss: 0.0386 - Mystery_loss: 0.0707 - Science_loss: 0.1076 - Literature_loss: 0.1056 - Fantasy_loss: 0.0819 - Historical_loss: 0.1172 - Fiction_loss: 0.0598 - Thriller_accuracy: 0.9708 - Classics_accuracy: 0.9559 - Romance_accuracy: 0.9873 - Mystery_accuracy: 0.9771 - Science_accuracy: 0.9551 - Literature_accuracy: 0.9500 - Fantasy_accuracy: 0.9708 - Historical_accuracy: 0.9555 - Fiction_accuracy: 0.9788 - val_loss: 5.0578 - val_Thriller_loss: 0.4603 - val_Classics_loss: 0.6660 - val_Romance_loss: 0.7986 - val_Mystery_loss: 0.4334 - val_Science_loss: 0.4745 - val_Literature_loss: 0.3962 - val_Fantasy_loss: 0.6681 - val_Historical_loss: 0.4622 - val_Fiction_loss: 0.6985 - val_Thriller_accuracy: 0.8966 - val_Classics_accuracy: 0.8373 - val_Romance_accuracy: 0.7983 - val_Mystery_accuracy: 0.8780 - val_Science_accuracy: 0.8305 - val_Literature_accuracy: 0.8831 - val_Fantasy_accuracy: 0.7678 - val_Historical_accuracy: 0.8288 - val_Fiction_accuracy: 0.7966\n",
      "Epoch 13/30\n",
      "74/74 [==============================] - 20s 265ms/step - loss: 0.6668 - Thriller_loss: 0.0735 - Classics_loss: 0.1110 - Romance_loss: 0.0334 - Mystery_loss: 0.0702 - Science_loss: 0.0925 - Literature_loss: 0.0891 - Fantasy_loss: 0.0530 - Historical_loss: 0.0870 - Fiction_loss: 0.0570 - Thriller_accuracy: 0.9720 - Classics_accuracy: 0.9483 - Romance_accuracy: 0.9890 - Mystery_accuracy: 0.9720 - Science_accuracy: 0.9640 - Literature_accuracy: 0.9640 - Fantasy_accuracy: 0.9831 - Historical_accuracy: 0.9686 - Fiction_accuracy: 0.9809 - val_loss: 4.9311 - val_Thriller_loss: 0.3926 - val_Classics_loss: 0.6162 - val_Romance_loss: 0.7393 - val_Mystery_loss: 0.4330 - val_Science_loss: 0.4611 - val_Literature_loss: 0.3455 - val_Fantasy_loss: 0.7052 - val_Historical_loss: 0.4690 - val_Fiction_loss: 0.7694 - val_Thriller_accuracy: 0.8966 - val_Classics_accuracy: 0.8271 - val_Romance_accuracy: 0.8017 - val_Mystery_accuracy: 0.8593 - val_Science_accuracy: 0.8339 - val_Literature_accuracy: 0.8847 - val_Fantasy_accuracy: 0.7678 - val_Historical_accuracy: 0.8254 - val_Fiction_accuracy: 0.8119\n",
      "Epoch 14/30\n",
      "74/74 [==============================] - 20s 265ms/step - loss: 0.6249 - Thriller_loss: 0.0779 - Classics_loss: 0.0880 - Romance_loss: 0.0472 - Mystery_loss: 0.0625 - Science_loss: 0.0799 - Literature_loss: 0.0834 - Fantasy_loss: 0.0546 - Historical_loss: 0.0723 - Fiction_loss: 0.0591 - Thriller_accuracy: 0.9699 - Classics_accuracy: 0.9610 - Romance_accuracy: 0.9860 - Mystery_accuracy: 0.9767 - Science_accuracy: 0.9708 - Literature_accuracy: 0.9644 - Fantasy_accuracy: 0.9780 - Historical_accuracy: 0.9746 - Fiction_accuracy: 0.9818 - val_loss: 5.5297 - val_Thriller_loss: 0.4219 - val_Classics_loss: 0.6466 - val_Romance_loss: 0.8635 - val_Mystery_loss: 0.4113 - val_Science_loss: 0.4783 - val_Literature_loss: 0.3591 - val_Fantasy_loss: 1.0191 - val_Historical_loss: 0.4863 - val_Fiction_loss: 0.8437 - val_Thriller_accuracy: 0.9017 - val_Classics_accuracy: 0.8271 - val_Romance_accuracy: 0.8051 - val_Mystery_accuracy: 0.8661 - val_Science_accuracy: 0.8356 - val_Literature_accuracy: 0.8864 - val_Fantasy_accuracy: 0.7203 - val_Historical_accuracy: 0.8169 - val_Fiction_accuracy: 0.8034\n",
      "Epoch 15/30\n",
      "74/74 [==============================] - 19s 257ms/step - loss: 0.5592 - Thriller_loss: 0.0575 - Classics_loss: 0.0822 - Romance_loss: 0.0385 - Mystery_loss: 0.0569 - Science_loss: 0.0727 - Literature_loss: 0.0774 - Fantasy_loss: 0.0541 - Historical_loss: 0.0667 - Fiction_loss: 0.0533 - Thriller_accuracy: 0.9763 - Classics_accuracy: 0.9661 - Romance_accuracy: 0.9873 - Mystery_accuracy: 0.9805 - Science_accuracy: 0.9720 - Literature_accuracy: 0.9691 - Fantasy_accuracy: 0.9784 - Historical_accuracy: 0.9784 - Fiction_accuracy: 0.9788 - val_loss: 5.1938 - val_Thriller_loss: 0.4274 - val_Classics_loss: 0.6274 - val_Romance_loss: 0.7586 - val_Mystery_loss: 0.4657 - val_Science_loss: 0.4726 - val_Literature_loss: 0.3943 - val_Fantasy_loss: 0.7861 - val_Historical_loss: 0.4628 - val_Fiction_loss: 0.7988 - val_Thriller_accuracy: 0.8966 - val_Classics_accuracy: 0.8339 - val_Romance_accuracy: 0.8288 - val_Mystery_accuracy: 0.8695 - val_Science_accuracy: 0.8356 - val_Literature_accuracy: 0.8797 - val_Fantasy_accuracy: 0.7644 - val_Historical_accuracy: 0.8356 - val_Fiction_accuracy: 0.7881\n",
      "Epoch 16/30\n",
      "74/74 [==============================] - 19s 256ms/step - loss: 0.4848 - Thriller_loss: 0.0651 - Classics_loss: 0.0634 - Romance_loss: 0.0272 - Mystery_loss: 0.0430 - Science_loss: 0.0702 - Literature_loss: 0.0587 - Fantasy_loss: 0.0437 - Historical_loss: 0.0625 - Fiction_loss: 0.0509 - Thriller_accuracy: 0.9742 - Classics_accuracy: 0.9746 - Romance_accuracy: 0.9903 - Mystery_accuracy: 0.9831 - Science_accuracy: 0.9746 - Literature_accuracy: 0.9763 - Fantasy_accuracy: 0.9856 - Historical_accuracy: 0.9780 - Fiction_accuracy: 0.9822 - val_loss: 5.7081 - val_Thriller_loss: 0.4869 - val_Classics_loss: 0.7315 - val_Romance_loss: 0.9623 - val_Mystery_loss: 0.4920 - val_Science_loss: 0.5412 - val_Literature_loss: 0.4155 - val_Fantasy_loss: 0.7644 - val_Historical_loss: 0.5000 - val_Fiction_loss: 0.8144 - val_Thriller_accuracy: 0.8983 - val_Classics_accuracy: 0.8288 - val_Romance_accuracy: 0.8169 - val_Mystery_accuracy: 0.8763 - val_Science_accuracy: 0.8322 - val_Literature_accuracy: 0.8814 - val_Fantasy_accuracy: 0.7729 - val_Historical_accuracy: 0.8407 - val_Fiction_accuracy: 0.8051\n",
      "Epoch 17/30\n",
      "74/74 [==============================] - 19s 255ms/step - loss: 0.4228 - Thriller_loss: 0.0623 - Classics_loss: 0.0640 - Romance_loss: 0.0305 - Mystery_loss: 0.0323 - Science_loss: 0.0566 - Literature_loss: 0.0548 - Fantasy_loss: 0.0349 - Historical_loss: 0.0458 - Fiction_loss: 0.0417 - Thriller_accuracy: 0.9737 - Classics_accuracy: 0.9767 - Romance_accuracy: 0.9877 - Mystery_accuracy: 0.9890 - Science_accuracy: 0.9746 - Literature_accuracy: 0.9809 - Fantasy_accuracy: 0.9860 - Historical_accuracy: 0.9847 - Fiction_accuracy: 0.9852 - val_loss: 5.7331 - val_Thriller_loss: 0.4529 - val_Classics_loss: 0.7535 - val_Romance_loss: 0.8717 - val_Mystery_loss: 0.4978 - val_Science_loss: 0.5860 - val_Literature_loss: 0.4169 - val_Fantasy_loss: 0.7658 - val_Historical_loss: 0.5219 - val_Fiction_loss: 0.8668 - val_Thriller_accuracy: 0.8966 - val_Classics_accuracy: 0.8220 - val_Romance_accuracy: 0.8169 - val_Mystery_accuracy: 0.8610 - val_Science_accuracy: 0.8085 - val_Literature_accuracy: 0.8797 - val_Fantasy_accuracy: 0.7763 - val_Historical_accuracy: 0.8322 - val_Fiction_accuracy: 0.7915\n",
      "Epoch 18/30\n",
      "74/74 [==============================] - 19s 262ms/step - loss: 0.4121 - Thriller_loss: 0.0575 - Classics_loss: 0.0596 - Romance_loss: 0.0333 - Mystery_loss: 0.0401 - Science_loss: 0.0435 - Literature_loss: 0.0550 - Fantasy_loss: 0.0320 - Historical_loss: 0.0418 - Fiction_loss: 0.0493 - Thriller_accuracy: 0.9763 - Classics_accuracy: 0.9771 - Romance_accuracy: 0.9890 - Mystery_accuracy: 0.9864 - Science_accuracy: 0.9860 - Literature_accuracy: 0.9775 - Fantasy_accuracy: 0.9877 - Historical_accuracy: 0.9856 - Fiction_accuracy: 0.9835 - val_loss: 5.8295 - val_Thriller_loss: 0.4478 - val_Classics_loss: 0.6925 - val_Romance_loss: 0.9064 - val_Mystery_loss: 0.5971 - val_Science_loss: 0.5514 - val_Literature_loss: 0.4185 - val_Fantasy_loss: 0.8007 - val_Historical_loss: 0.5685 - val_Fiction_loss: 0.8466 - val_Thriller_accuracy: 0.9000 - val_Classics_accuracy: 0.8186 - val_Romance_accuracy: 0.8153 - val_Mystery_accuracy: 0.8661 - val_Science_accuracy: 0.8356 - val_Literature_accuracy: 0.8847 - val_Fantasy_accuracy: 0.7864 - val_Historical_accuracy: 0.8288 - val_Fiction_accuracy: 0.7576\n",
      "Epoch 19/30\n",
      "74/74 [==============================] - 18s 247ms/step - loss: 0.3732 - Thriller_loss: 0.0568 - Classics_loss: 0.0547 - Romance_loss: 0.0290 - Mystery_loss: 0.0332 - Science_loss: 0.0443 - Literature_loss: 0.0391 - Fantasy_loss: 0.0295 - Historical_loss: 0.0461 - Fiction_loss: 0.0404 - Thriller_accuracy: 0.9797 - Classics_accuracy: 0.9801 - Romance_accuracy: 0.9877 - Mystery_accuracy: 0.9886 - Science_accuracy: 0.9873 - Literature_accuracy: 0.9852 - Fantasy_accuracy: 0.9911 - Historical_accuracy: 0.9839 - Fiction_accuracy: 0.9847 - val_loss: 5.5177 - val_Thriller_loss: 0.3805 - val_Classics_loss: 0.5860 - val_Romance_loss: 0.8206 - val_Mystery_loss: 0.4950 - val_Science_loss: 0.5822 - val_Literature_loss: 0.3645 - val_Fantasy_loss: 0.7522 - val_Historical_loss: 0.7086 - val_Fiction_loss: 0.8280 - val_Thriller_accuracy: 0.8949 - val_Classics_accuracy: 0.8102 - val_Romance_accuracy: 0.8085 - val_Mystery_accuracy: 0.8576 - val_Science_accuracy: 0.8237 - val_Literature_accuracy: 0.8898 - val_Fantasy_accuracy: 0.7864 - val_Historical_accuracy: 0.7441 - val_Fiction_accuracy: 0.8034\n",
      "Epoch 20/30\n",
      "74/74 [==============================] - 19s 263ms/step - loss: 0.3451 - Thriller_loss: 0.0593 - Classics_loss: 0.0544 - Romance_loss: 0.0188 - Mystery_loss: 0.0225 - Science_loss: 0.0418 - Literature_loss: 0.0359 - Fantasy_loss: 0.0275 - Historical_loss: 0.0453 - Fiction_loss: 0.0396 - Thriller_accuracy: 0.9763 - Classics_accuracy: 0.9775 - Romance_accuracy: 0.9945 - Mystery_accuracy: 0.9928 - Science_accuracy: 0.9852 - Literature_accuracy: 0.9877 - Fantasy_accuracy: 0.9907 - Historical_accuracy: 0.9831 - Fiction_accuracy: 0.9864 - val_loss: 6.1582 - val_Thriller_loss: 0.5207 - val_Classics_loss: 0.7565 - val_Romance_loss: 0.8736 - val_Mystery_loss: 0.6100 - val_Science_loss: 0.6029 - val_Literature_loss: 0.4657 - val_Fantasy_loss: 0.8855 - val_Historical_loss: 0.5957 - val_Fiction_loss: 0.8475 - val_Thriller_accuracy: 0.8915 - val_Classics_accuracy: 0.8102 - val_Romance_accuracy: 0.8169 - val_Mystery_accuracy: 0.8644 - val_Science_accuracy: 0.8356 - val_Literature_accuracy: 0.8847 - val_Fantasy_accuracy: 0.7898 - val_Historical_accuracy: 0.8288 - val_Fiction_accuracy: 0.7814\n",
      "Epoch 21/30\n",
      "74/74 [==============================] - 20s 269ms/step - loss: 0.3238 - Thriller_loss: 0.0486 - Classics_loss: 0.0449 - Romance_loss: 0.0204 - Mystery_loss: 0.0236 - Science_loss: 0.0346 - Literature_loss: 0.0414 - Fantasy_loss: 0.0271 - Historical_loss: 0.0348 - Fiction_loss: 0.0486 - Thriller_accuracy: 0.9809 - Classics_accuracy: 0.9831 - Romance_accuracy: 0.9924 - Mystery_accuracy: 0.9915 - Science_accuracy: 0.9856 - Literature_accuracy: 0.9852 - Fantasy_accuracy: 0.9890 - Historical_accuracy: 0.9881 - Fiction_accuracy: 0.9831 - val_loss: 6.2138 - val_Thriller_loss: 0.4968 - val_Classics_loss: 0.7849 - val_Romance_loss: 0.9893 - val_Mystery_loss: 0.6070 - val_Science_loss: 0.6052 - val_Literature_loss: 0.4469 - val_Fantasy_loss: 0.8174 - val_Historical_loss: 0.5677 - val_Fiction_loss: 0.8987 - val_Thriller_accuracy: 0.8966 - val_Classics_accuracy: 0.8051 - val_Romance_accuracy: 0.8085 - val_Mystery_accuracy: 0.8661 - val_Science_accuracy: 0.8119 - val_Literature_accuracy: 0.8864 - val_Fantasy_accuracy: 0.7847 - val_Historical_accuracy: 0.8339 - val_Fiction_accuracy: 0.7746\n",
      "Epoch 22/30\n",
      "74/74 [==============================] - 19s 259ms/step - loss: 0.3008 - Thriller_loss: 0.0427 - Classics_loss: 0.0439 - Romance_loss: 0.0172 - Mystery_loss: 0.0275 - Science_loss: 0.0424 - Literature_loss: 0.0356 - Fantasy_loss: 0.0185 - Historical_loss: 0.0308 - Fiction_loss: 0.0422 - Thriller_accuracy: 0.9805 - Classics_accuracy: 0.9826 - Romance_accuracy: 0.9936 - Mystery_accuracy: 0.9894 - Science_accuracy: 0.9835 - Literature_accuracy: 0.9873 - Fantasy_accuracy: 0.9945 - Historical_accuracy: 0.9911 - Fiction_accuracy: 0.9847 - val_loss: 6.6456 - val_Thriller_loss: 0.5407 - val_Classics_loss: 0.8161 - val_Romance_loss: 0.9174 - val_Mystery_loss: 0.6509 - val_Science_loss: 0.6635 - val_Literature_loss: 0.5092 - val_Fantasy_loss: 0.9464 - val_Historical_loss: 0.6583 - val_Fiction_loss: 0.9430 - val_Thriller_accuracy: 0.8932 - val_Classics_accuracy: 0.8186 - val_Romance_accuracy: 0.8051 - val_Mystery_accuracy: 0.8695 - val_Science_accuracy: 0.8339 - val_Literature_accuracy: 0.8695 - val_Fantasy_accuracy: 0.7847 - val_Historical_accuracy: 0.8085 - val_Fiction_accuracy: 0.7610\n",
      "Epoch 23/30\n",
      "74/74 [==============================] - 19s 253ms/step - loss: 0.2821 - Thriller_loss: 0.0405 - Classics_loss: 0.0374 - Romance_loss: 0.0184 - Mystery_loss: 0.0243 - Science_loss: 0.0320 - Literature_loss: 0.0298 - Fantasy_loss: 0.0238 - Historical_loss: 0.0362 - Fiction_loss: 0.0398 - Thriller_accuracy: 0.9831 - Classics_accuracy: 0.9869 - Romance_accuracy: 0.9941 - Mystery_accuracy: 0.9911 - Science_accuracy: 0.9881 - Literature_accuracy: 0.9886 - Fantasy_accuracy: 0.9932 - Historical_accuracy: 0.9877 - Fiction_accuracy: 0.9839 - val_loss: 6.2001 - val_Thriller_loss: 0.4864 - val_Classics_loss: 0.7267 - val_Romance_loss: 0.9027 - val_Mystery_loss: 0.6319 - val_Science_loss: 0.6046 - val_Literature_loss: 0.4561 - val_Fantasy_loss: 0.8274 - val_Historical_loss: 0.6329 - val_Fiction_loss: 0.9315 - val_Thriller_accuracy: 0.8915 - val_Classics_accuracy: 0.8102 - val_Romance_accuracy: 0.8220 - val_Mystery_accuracy: 0.8525 - val_Science_accuracy: 0.8424 - val_Literature_accuracy: 0.8864 - val_Fantasy_accuracy: 0.7864 - val_Historical_accuracy: 0.8085 - val_Fiction_accuracy: 0.7695\n",
      "Epoch 24/30\n",
      "74/74 [==============================] - 20s 272ms/step - loss: 0.2704 - Thriller_loss: 0.0497 - Classics_loss: 0.0433 - Romance_loss: 0.0168 - Mystery_loss: 0.0218 - Science_loss: 0.0338 - Literature_loss: 0.0297 - Fantasy_loss: 0.0176 - Historical_loss: 0.0239 - Fiction_loss: 0.0338 - Thriller_accuracy: 0.9784 - Classics_accuracy: 0.9856 - Romance_accuracy: 0.9936 - Mystery_accuracy: 0.9932 - Science_accuracy: 0.9886 - Literature_accuracy: 0.9886 - Fantasy_accuracy: 0.9936 - Historical_accuracy: 0.9928 - Fiction_accuracy: 0.9856 - val_loss: 6.1854 - val_Thriller_loss: 0.5061 - val_Classics_loss: 0.7707 - val_Romance_loss: 0.8943 - val_Mystery_loss: 0.6386 - val_Science_loss: 0.5743 - val_Literature_loss: 0.4244 - val_Fantasy_loss: 0.9296 - val_Historical_loss: 0.5922 - val_Fiction_loss: 0.8551 - val_Thriller_accuracy: 0.8814 - val_Classics_accuracy: 0.8153 - val_Romance_accuracy: 0.8288 - val_Mystery_accuracy: 0.8593 - val_Science_accuracy: 0.8271 - val_Literature_accuracy: 0.8915 - val_Fantasy_accuracy: 0.7797 - val_Historical_accuracy: 0.8237 - val_Fiction_accuracy: 0.7661\n",
      "Epoch 25/30\n",
      "74/74 [==============================] - 19s 251ms/step - loss: 0.2732 - Thriller_loss: 0.0448 - Classics_loss: 0.0509 - Romance_loss: 0.0132 - Mystery_loss: 0.0190 - Science_loss: 0.0294 - Literature_loss: 0.0284 - Fantasy_loss: 0.0315 - Historical_loss: 0.0292 - Fiction_loss: 0.0267 - Thriller_accuracy: 0.9805 - Classics_accuracy: 0.9818 - Romance_accuracy: 0.9975 - Mystery_accuracy: 0.9919 - Science_accuracy: 0.9890 - Literature_accuracy: 0.9886 - Fantasy_accuracy: 0.9864 - Historical_accuracy: 0.9915 - Fiction_accuracy: 0.9886 - val_loss: 6.6060 - val_Thriller_loss: 0.5317 - val_Classics_loss: 0.7756 - val_Romance_loss: 1.0516 - val_Mystery_loss: 0.6234 - val_Science_loss: 0.6316 - val_Literature_loss: 0.4623 - val_Fantasy_loss: 0.9796 - val_Historical_loss: 0.6855 - val_Fiction_loss: 0.8648 - val_Thriller_accuracy: 0.8898 - val_Classics_accuracy: 0.8068 - val_Romance_accuracy: 0.8169 - val_Mystery_accuracy: 0.8644 - val_Science_accuracy: 0.8424 - val_Literature_accuracy: 0.8797 - val_Fantasy_accuracy: 0.7610 - val_Historical_accuracy: 0.7881 - val_Fiction_accuracy: 0.7864\n",
      "Epoch 26/30\n",
      "74/74 [==============================] - 20s 272ms/step - loss: 0.2251 - Thriller_loss: 0.0395 - Classics_loss: 0.0310 - Romance_loss: 0.0129 - Mystery_loss: 0.0179 - Science_loss: 0.0219 - Literature_loss: 0.0263 - Fantasy_loss: 0.0167 - Historical_loss: 0.0288 - Fiction_loss: 0.0300 - Thriller_accuracy: 0.9822 - Classics_accuracy: 0.9890 - Romance_accuracy: 0.9962 - Mystery_accuracy: 0.9941 - Science_accuracy: 0.9919 - Literature_accuracy: 0.9911 - Fantasy_accuracy: 0.9941 - Historical_accuracy: 0.9903 - Fiction_accuracy: 0.9881 - val_loss: 6.5657 - val_Thriller_loss: 0.5203 - val_Classics_loss: 0.8237 - val_Romance_loss: 1.0006 - val_Mystery_loss: 0.6324 - val_Science_loss: 0.6094 - val_Literature_loss: 0.4813 - val_Fantasy_loss: 0.9353 - val_Historical_loss: 0.6926 - val_Fiction_loss: 0.8701 - val_Thriller_accuracy: 0.8932 - val_Classics_accuracy: 0.8254 - val_Romance_accuracy: 0.8102 - val_Mystery_accuracy: 0.8695 - val_Science_accuracy: 0.8305 - val_Literature_accuracy: 0.8814 - val_Fantasy_accuracy: 0.7831 - val_Historical_accuracy: 0.8051 - val_Fiction_accuracy: 0.7763\n",
      "Epoch 27/30\n",
      "74/74 [==============================] - 19s 252ms/step - loss: 0.2346 - Thriller_loss: 0.0435 - Classics_loss: 0.0325 - Romance_loss: 0.0216 - Mystery_loss: 0.0134 - Science_loss: 0.0166 - Literature_loss: 0.0314 - Fantasy_loss: 0.0131 - Historical_loss: 0.0318 - Fiction_loss: 0.0307 - Thriller_accuracy: 0.9822 - Classics_accuracy: 0.9852 - Romance_accuracy: 0.9924 - Mystery_accuracy: 0.9962 - Science_accuracy: 0.9953 - Literature_accuracy: 0.9903 - Fantasy_accuracy: 0.9953 - Historical_accuracy: 0.9881 - Fiction_accuracy: 0.9907 - val_loss: 6.9682 - val_Thriller_loss: 0.5053 - val_Classics_loss: 0.7677 - val_Romance_loss: 1.1309 - val_Mystery_loss: 0.7151 - val_Science_loss: 0.6855 - val_Literature_loss: 0.5651 - val_Fantasy_loss: 0.9777 - val_Historical_loss: 0.6292 - val_Fiction_loss: 0.9917 - val_Thriller_accuracy: 0.8898 - val_Classics_accuracy: 0.8000 - val_Romance_accuracy: 0.8051 - val_Mystery_accuracy: 0.8661 - val_Science_accuracy: 0.8153 - val_Literature_accuracy: 0.8831 - val_Fantasy_accuracy: 0.7712 - val_Historical_accuracy: 0.8119 - val_Fiction_accuracy: 0.7797\n",
      "Epoch 28/30\n",
      "74/74 [==============================] - 19s 255ms/step - loss: 0.2288 - Thriller_loss: 0.0397 - Classics_loss: 0.0392 - Romance_loss: 0.0131 - Mystery_loss: 0.0158 - Science_loss: 0.0271 - Literature_loss: 0.0213 - Fantasy_loss: 0.0155 - Historical_loss: 0.0324 - Fiction_loss: 0.0247 - Thriller_accuracy: 0.9831 - Classics_accuracy: 0.9860 - Romance_accuracy: 0.9962 - Mystery_accuracy: 0.9945 - Science_accuracy: 0.9915 - Literature_accuracy: 0.9919 - Fantasy_accuracy: 0.9936 - Historical_accuracy: 0.9886 - Fiction_accuracy: 0.9932 - val_loss: 6.6194 - val_Thriller_loss: 0.4578 - val_Classics_loss: 0.7626 - val_Romance_loss: 1.0598 - val_Mystery_loss: 0.6621 - val_Science_loss: 0.6447 - val_Literature_loss: 0.4525 - val_Fantasy_loss: 0.9286 - val_Historical_loss: 0.7033 - val_Fiction_loss: 0.9480 - val_Thriller_accuracy: 0.8797 - val_Classics_accuracy: 0.8186 - val_Romance_accuracy: 0.8085 - val_Mystery_accuracy: 0.8424 - val_Science_accuracy: 0.8339 - val_Literature_accuracy: 0.8780 - val_Fantasy_accuracy: 0.7763 - val_Historical_accuracy: 0.7695 - val_Fiction_accuracy: 0.7881\n",
      "Epoch 29/30\n",
      "74/74 [==============================] - 20s 269ms/step - loss: 0.1924 - Thriller_loss: 0.0291 - Classics_loss: 0.0329 - Romance_loss: 0.0103 - Mystery_loss: 0.0165 - Science_loss: 0.0222 - Literature_loss: 0.0248 - Fantasy_loss: 0.0192 - Historical_loss: 0.0168 - Fiction_loss: 0.0206 - Thriller_accuracy: 0.9894 - Classics_accuracy: 0.9877 - Romance_accuracy: 0.9975 - Mystery_accuracy: 0.9958 - Science_accuracy: 0.9924 - Literature_accuracy: 0.9932 - Fantasy_accuracy: 0.9928 - Historical_accuracy: 0.9953 - Fiction_accuracy: 0.9928 - val_loss: 7.2038 - val_Thriller_loss: 0.5517 - val_Classics_loss: 0.8555 - val_Romance_loss: 1.0258 - val_Mystery_loss: 0.7053 - val_Science_loss: 0.7165 - val_Literature_loss: 0.5456 - val_Fantasy_loss: 1.0401 - val_Historical_loss: 0.7493 - val_Fiction_loss: 1.0141 - val_Thriller_accuracy: 0.8915 - val_Classics_accuracy: 0.8000 - val_Romance_accuracy: 0.8085 - val_Mystery_accuracy: 0.8475 - val_Science_accuracy: 0.8254 - val_Literature_accuracy: 0.8847 - val_Fantasy_accuracy: 0.7712 - val_Historical_accuracy: 0.7915 - val_Fiction_accuracy: 0.7864\n",
      "Epoch 30/30\n",
      "74/74 [==============================] - 19s 256ms/step - loss: 0.1886 - Thriller_loss: 0.0310 - Classics_loss: 0.0285 - Romance_loss: 0.0158 - Mystery_loss: 0.0141 - Science_loss: 0.0250 - Literature_loss: 0.0198 - Fantasy_loss: 0.0131 - Historical_loss: 0.0195 - Fiction_loss: 0.0217 - Thriller_accuracy: 0.9877 - Classics_accuracy: 0.9881 - Romance_accuracy: 0.9953 - Mystery_accuracy: 0.9958 - Science_accuracy: 0.9903 - Literature_accuracy: 0.9924 - Fantasy_accuracy: 0.9949 - Historical_accuracy: 0.9936 - Fiction_accuracy: 0.9932 - val_loss: 7.5391 - val_Thriller_loss: 0.5526 - val_Classics_loss: 0.9142 - val_Romance_loss: 1.1046 - val_Mystery_loss: 0.6918 - val_Science_loss: 0.7376 - val_Literature_loss: 0.5259 - val_Fantasy_loss: 1.2002 - val_Historical_loss: 0.7325 - val_Fiction_loss: 1.0798 - val_Thriller_accuracy: 0.8831 - val_Classics_accuracy: 0.8169 - val_Romance_accuracy: 0.8051 - val_Mystery_accuracy: 0.8627 - val_Science_accuracy: 0.8305 - val_Literature_accuracy: 0.8814 - val_Fantasy_accuracy: 0.7661 - val_Historical_accuracy: 0.7898 - val_Fiction_accuracy: 0.7898\n"
     ]
    }
   ],
   "source": [
    "# modeling \n",
    "\n",
    "# max length for GRU and pading sequence\n",
    "max_len = 100 \n",
    "x_pading = pad_sequences(x_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# labels\n",
    "y = train_data[['Thriller', 'Classics', 'Romance', 'Mystery', 'Science', 'Literature', 'Fantasy', 'Historical', 'Fiction']]\n",
    "\n",
    "# split the train data\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_pading, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# attention layer\n",
    "def attention_layer(hidden_states):\n",
    "    # Score computation\n",
    "    score = Dense(1, activation='tanh')(hidden_states) \n",
    "    attention_weights = tf.nn.softmax(score, axis=1) \n",
    "    \n",
    "    context_vector = tf.reduce_sum(attention_weights * hidden_states, axis=1) \n",
    "    \n",
    "    return context_vector\n",
    "\n",
    "# input layer\n",
    "inputs = Input(shape=(max_len,))\n",
    "\n",
    "# embedding and GRU layers\n",
    "x = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_len)(inputs)\n",
    "x = GRU(128, return_sequences=True)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# x = GRU(128, return_sequences=False)(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "x = attention_layer(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# output layer for each label, treating each as a separate classification task\n",
    "outputs = {}\n",
    "for label in y.columns:\n",
    "    outputs[label] = Dense(units=len(y[label].unique()), activation='softmax', name=label)(x)\n",
    "\n",
    "# model\n",
    "model = Model(inputs=inputs, outputs=list(outputs.values()))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# training and validation label data as dictionaries\n",
    "train_outputs = {label: y_train[label].values for label in y.columns}\n",
    "val_outputs = {label: y_val[label].values for label in y.columns}\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, train_outputs, epochs=30, batch_size=32, validation_data=(X_val, val_outputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da5d2a",
   "metadata": {},
   "source": [
    "<h3 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معیار ارزیابی\n",
    "</font>\n",
    "</h3>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    معیاری که برای ارزیابی عملکرد مدل انتخاب کرده‌ایم، <code>f1_score</code> نام دارد.\n",
    "    <br>\n",
    "    این معیار، سنجه ارزیابی کیفیت مدل شماست. به عبارت بهتر در سامانه داوری هم از همین معیار برای نمره‌دهی استفاده شده است.\n",
    "    <br>\n",
    "    این معیار برای هر مورد در ستون هدف به صورت جداگانه محاسبه شده و میانگین آن‌ها به عنوان امتیاز این مسئله در نظر گرفته می‌شود.\n",
    "    <br>\n",
    "    پیشنهاد می‌شود با توجه به این معیار، عملکرد مدل خود را بر روی مجموعه داده آموزش یا اعتبارسنجی ارزیابی کنید.\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cee1b883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 44ms/step\n",
      "F1-Score for Thriller: 0.8772507930108563\n",
      "F1-Score for Classics: 0.8115954073264078\n",
      "F1-Score for Romance: 0.8024149576379579\n",
      "F1-Score for Mystery: 0.858785084377453\n",
      "F1-Score for Science: 0.8305084745762712\n",
      "F1-Score for Literature: 0.8724684279051178\n",
      "F1-Score for Fantasy: 0.7677687198335819\n",
      "F1-Score for Historical: 0.8006390413630116\n",
      "F1-Score for Fiction: 0.7713279173265474\n",
      "Final score is:  164.2\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "from sklearn.metrics import  f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# evaluate the model\n",
    "val_pred = model.predict(X_val)\n",
    "\n",
    "# F1-scores for each label\n",
    "scores = {}\n",
    "for i, label in enumerate(y.columns):\n",
    "    val_pred_label = val_pred[i].argmax(axis=1) \n",
    "    score = f1_score(y_val[label], val_pred_label, average='weighted')\n",
    "    scores[label] = score\n",
    "    print(f'F1-Score for {label}:', score)\n",
    "\n",
    "# final score\n",
    "final_score = round(np.mean(list(scores.values())), 3) * 200\n",
    "print(\"Final score is: \", final_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e9984de",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    " پیش‌بینی بر اساس داده‌های ورودی و خروجی مورد انتظار\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: right;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    پس از انجام فرآیند مهندسی ویژگی و مدل‌سازی، اکنون الگوریتمی در اختیار دارید که قادر است از داده‌های مستقل به نتایج مورد نظر دست یابد.\n",
    "    <br>\n",
    "    لطفاً از مدل خود برای پیش‌بینی برچسب دسته‌بندی کتاب‌های موجود در داده تست استفاده کنید و خروجی‌ها را در قالب یک جدول (<code>dataframe</code>) آماده کنید. داده‌های تست به صورت زیر خواهند بود:\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<div dir=rtl style=\"direction: rtl;text-align: right;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "|ستون|توضیحات|\n",
    "|:------:|:---:|\n",
    "|Title|عنوان کتاب|\n",
    "|Description|توضیحات مرتبط با کتاب|\n",
    "    \n",
    "</font>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6d82d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: right;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    اسم دیتافریم باید <i>submission</i> باشد؛ در غیر این صورت، سامانه داوری نمی‌تواند تلاش شما را ارزیابی کند.\n",
    "    <br>این دیتافریم باید شامل ۹ ستون باشد که هر کدام نمایانگر برچسب دسته‌بندی کتاب است و ۷۳۹ سطر دارد.\n",
    "    <br>\n",
    "    به ازای هر سطر موجود در دیتافریم <i>test</i> شما باید مقدار پیشبینی شده برای هر برچسب‌ داشته باشید.\n",
    "    <br>\n",
    "    جدول زیر، ۵ سطر ابتدایی دیتافریم <code>submission</code> را نشان می‌دهد. البته در جواب شما، مقادیر ممکن است متفاوت باشد.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<div style=\"text-align: center;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "| Thriller | Classics | Romance | Mystery | Science | Literature | Fantasy | Historical | Fiction |\n",
    "|:--------:|:--------:|:-------:|:-------:|:-------:|:----------:|:-------:|:----------:|:-------:|\n",
    "|    0     |     1    |    0    |    0    |    0    |      0     |    0    |      1     |    1    |\n",
    "|    0     |     0    |    1    |    0    |    0    |      0     |    1    |      0     |    0    |\n",
    "|    0     |     0    |    1    |    0    |    0    |      0     |    1    |      0     |    1    |\n",
    "|    1     |     0    |    0    |    0    |    0    |      0     |    0    |      1     |    1    |\n",
    "|    0     |     0    |    1    |    0    |    0    |      0     |    0    |      0     |    0    |\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d4a0844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test pred:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Classics</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Science</th>\n",
       "      <th>Literature</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Historical</th>\n",
       "      <th>Fiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Thriller  Classics  Romance  Mystery  Science  Literature  Fantasy  \\\n",
       "0           0         1        0        0        0           0        1   \n",
       "1           0         0        1        0        0           0        1   \n",
       "2           0         0        0        1        0           0        0   \n",
       "3           0         0        1        0        0           0        1   \n",
       "4           0         0        1        0        0           0        1   \n",
       "..        ...       ...      ...      ...      ...         ...      ...   \n",
       "734         0         0        1        0        0           0        0   \n",
       "735         0         0        0        0        0           0        1   \n",
       "736         0         0        0        0        0           0        0   \n",
       "737         0         0        1        0        0           0        1   \n",
       "738         1         0        0        1        0           0        0   \n",
       "\n",
       "     Historical  Fiction  \n",
       "0             1        1  \n",
       "1             0        1  \n",
       "2             0        1  \n",
       "3             1        1  \n",
       "4             0        0  \n",
       "..          ...      ...  \n",
       "734           0        0  \n",
       "735           0        1  \n",
       "736           0        1  \n",
       "737           0        1  \n",
       "738           0        1  \n",
       "\n",
       "[739 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict test samples\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_data['description'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "test_pred = model.predict(X_test_pad, verbose=0)\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "# Convert predictions to the actual class labels\n",
    "for i, label in enumerate(y.columns):\n",
    "    submission[label] = test_pred[i].argmax(axis=1)\n",
    "\n",
    "print(\"\\nTest pred:\")\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77e7cd",
   "metadata": {},
   "source": [
    "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "<b>سلول جواب‌ساز</b>\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای ساخته‌شدن فایل <code>result.zip</code> سلول زیر را اجرا کنید. توجه داشته باشید که پیش از اجرای سلول زیر تغییرات اعمال شده در نت‌بوک را ذخیره کرده باشید (<code>ctrl+s</code>) تا در صورت نیاز به پشتیبانی امکان بررسی کد شما وجود داشته باشد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c3bcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Paths:\n",
      "['Babel.ipynb', 'submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "            \n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['Babel.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625fa62-8aaa-459a-b550-b183fc366d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
